---
layout: page
title: Call for Papers
permalink: /cfp2026/
---

<!--
{::options parse_block_html="true" /}
<div class="box">
**Important notice:** The camera-ready due will be the same as the ACMMM regular paper.
</div>
-->


ACM Multimedia Reproducibility (ACMMM Repro) Overview and Guidelines [here]({{site.baseurl}}/).
The page gives the instructions for submitting reproducibility companion papers for the ACM Multimedia 2026. 

* TOC
{:toc}

## Dates 
The important dates along the timeline for submitting the companion paper and the associated artifacts are:

- March 18, 2026: deadline for submitting __title__ to [OpenReview](https://openreview.net/group?id=acmmm.org/ACMMM/2026/Reproducibility_Track).
- April 1, 2026: deadline for submitting the companion paper and reproducibility archive to OpenReview. 
- July 9, 2026: notification of acceptance/rejection
- August 6, 2026: deadline for preparing the final version of the accepted companion paper
- November 10-14, 2026: present your reproducibility work at a poster session during the 2026 edition of ACM MM in Rio de Janeiro, Brazil.

## Program Committee

**Chairs**:

- Marc A. Kastner, Hiroshima City University, Japan
- Luca Rosetto, Dublin City University, Ireland
  
<!--
**Committee members**:

- TBA
-->


## Submission

A submission has essentially two parts:

* **Companion Paper (ACMMM latest format of PDF file)**: The companion paper is 2–3.5 pages in length (with an optional page for references). It contains a overview description of the contributions and an introduction of the experiments carried out in the original paper and that are implemented in the submitted archive. It must follow the standard ACM style format, double column. It has to involve a majority of the authors of the original paper and provide in the clear their names and affiliations. 

* **Archive (github repository and a compressed zip file)**: Contains all the artifacts and information (code, scripts, datasets, and protocols, examples, along with detailed README with instructions) for reproducing the results. The final submission should be finalized, packaged, ready to download, and reproduce the results in a one-click execution.

# Submission Guidelines

Authors that have a main-conference paper published at ACM Multimedia 2024 and 2025 are invited to submit a short reproducibility companion paper to the ACM Multimedia Reproducibility track at ACM Multimedia 2026. That companion paper typically focuses on the technical details of what you published at ACM MM 2024/2025.

The companion paper should be submitted as a short paper that is 2–3.5 pages long, excluding references. It must follow the standard ACM style format, double column. It has to involve a majority of the authors of the original paper and provide in the clear their names and affiliations. The original ACM Multimedia 2024/2025 contribution associated with this companion paper must be clearly referenced.

The reproducibility companion paper and the associated artifacts will undergo a reproducibility review, which will result in an accept or a reject decision. Rejected papers are not disclosed.

If accepted, a Results Reproduced ACM badge is added to the original paper and to the companion paper, which are both stored in the ACM Digital Library, together with the artifacts. A badged companion paper will appear in the ACM Multimedia 2025 proceedings and will be presented as a Reproducibility poster in the ACM Multimedia 2025 Reproducibility poster session. The reviewers of the badged companion paper add a section documenting their efforts and become co-authors of the paper. The final version may be up to 4 pages (with an optional page for references).

If, during the evaluation, a serious flaw invalidating the scientific results published in the original contribution is discovered, then the companion paper is rejected, and the authors are encouraged to publish an errata.

Note: If your original paper turns out to be especially challenging to replicate, the committee will recommend that the companion paper be published at ACM Multimedia 2026, instead of ACM Multimedia 2025. If replication cannot be completed in time for ACM Multimedia 2026, then the paper cannot be accepted.

---

# Reproducibility Certification

## Paper Badge

In 2025, we are again committed to set the standard high for reproducibility at ACM Multimedia. Thus the Reproducibility Committee asks authors to target a top-quality badge for the companion papers they submit. The target is the Results Reproduced badge:

![](https://www.acm.org/binaries/content/gallery/acm/publications/replication-badges/results_reproduced_dl.jpg)

ACM gives the following definition of this Results Reproduced badge:

> The main results of the paper have been obtained in a subsequent study by a person or team other than the authors, using, in part, artifacts provided by the author.  <br /> &raquo;  ACM DL. [Read it here](https://www.acm.org/publications/policies/artifact-review-and-badging-current).


## Contents of the Companion Paper

The companion paper must provide means for the committee to download the artifacts that will enable replicating the findings that are in the original ACM Multimedia contribution. The author-created artifacts that are relevant to this paper must have been placed on a publicly accessible archival repository (e.g. GitHub).

The companion paper must describe the procedure allowing reviewers to easily find their way in the artifacts. That procedure might for example explain the reasons for having organized the artifacts in hierarchies of folders. It might specify what items inside the artifacts must be read, and in which order, what are the main elements to fully review first. It might clearly establish relationships between items in the artifacts and the corresponding elements that can be found in the original scientific contribution. It might explain and justify why this or that part of the original paper is not replicable.

Ideally, the companion paper should include text/schemas/illustrations that describe what the artifacts contain and how it should be deployed and then used. The companion paper should also contain notes about parameters that can be set or adjusted and about how to recreate the plots. It has to have examples, with comments. You can of course create documents in the archive that provide additional text/schemas/illustrations. In this case, the companion paper should clearly indicate where that can be found in the artifacts.


## Contents of the Archive of Artifacts
Replicability is grounded in the code, scripts and datasets that you provide, also called “artifacts”. More formally, ACM defines artifacts as follows:

> By “artifact” we mean a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.  <br /> &raquo;  ACM DL. [Read it here](https://www.acm.org/publications/policies/artifact-review-and-badging-current).

Artifacts contain digital objects that supplement the companion paper. Artifacts are typically a series of files, possibly organized according to a clear and easy to grasp hierarchy. Artifacts include for example:

1. The configuration files and scripts to set up and deploy the environment needed for the reviewers to subsequently run your code,
2. The source code if you expose your system as a white box,
3. Input Data: Either the process to generate the input data should be made available, or when the data is not generated, the actual data itself or a link to the data should be provided,
4. The set of experiments (system configuration and initialization, scripts, workload, measurement protocol, …) used to run the experiments that produce the raw experimental data,
5. The scripts needed to transform the raw experimental data into the graphs, tables, plots, …, that can be found in the original submission already published in the proceedings of ACM MM.

All this material should be extensively described, documented, commented, easy to understand, for example in specific files coming together with what they describe.

Note: We strongly recommend to expose your system as a white box so that anyone can replicate and reuse your results. If the system is only made accessible as a black box, however, reviewers may still require confidential access to source code, such that they can properly assess the validity of the reproducibility results.


### About experiments

The central results and claims of the corresponding published paper should be supported by the submitted experiments, meaning we can recreate result data and graphs that demonstrate similar behavior with that shown in that paper. Typically when the results are about response times, we do not expect to get identical results. Instead, we expect to see that the overall behavior matches the conclusions from the paper, e.g., that a given algorithm is significantly faster than another one, or that a given parameter affects negatively or positively the behavior of a system.

Given a system, the authors should provide a complete set of experiments to replicate the results that are in the original paper. Typically, each experiment will consist of the following parts.

- A setup phase where parameters are configured and data is loaded,
- A running phase where a workload is applied and measurements are taken,
- A clean-up phase where the system is prepared to avoid interference with the next round of experiments.

The authors should document (i) how to perform the setup, running and clean-up phases, and (ii) how to check that these phases complete as they should. The authors should document the expected effect of the setup phase (e.g., a cold file cache is enforced) and the different steps of the running phase, e.g., by documenting the combination of command-line options used to run a given experiment script.

Each experiment should be automatic, e.g., via a script that takes a range of values for each experiment parameter as arguments, rather than manual, e.g., via a script that must be edited so that a constant takes the value of a given experiment parameter.

We do not expect the authors to perform any additional experiments on top of the ones in their original paper. Any additional experiments submitted will be considered and tested but they are not required.

### About Graphs and Plots

For each graph/plot in the original paper, the authors should describe how the graph/plot is obtained from the experimental measurements. The submission should contain the scripts (or spreadsheets) that are used to generate the graphs/plots. We strongly encourage authors to provide scripts for all their graphs using a tool such as Gnuplot or Matplotlib. Here are two useful tutorials for Gnuplot: [a brief manual and tutorial](http://people.duke.edu/~hpgavin/gnuplot.html), and another two for Matplotlib: [examples from SciPy](https://scipy-cookbook.readthedocs.io/items/idx_matplotlib.html), and [a step-by-step tutorial discussing many features](http://www.labri.fr/perso/nrougier/teaching/matplotlib/).

Similar procedures must be provided by the authors in order to create the tables that are in the original paper.

### Ideal archive of artifacts

Authors are encouraged to strive for this ideal submission for truly replicable work… At a minimum, the authors should provide a complete set of scripts to install the system, produce the data, run experiments and produce the resulting graphs along with detailed readme file(s) that describe the process step by step so it can be easily redone by a reviewer. The ideal submission consists of extremely careful and detailed descriptions of the experiments, their parameters, and of a master script that:

1. installs all systems needed,
2. generates or fetches all needed input data,
3. reruns all experiments and generates all results,
4. generates all graphs, plots, and tables, and finally,
5. recompiles the sources of the paper

… to produce a brand new PDF for the paper with all the replicated experiments. It’ll allow comparing this replicated material with the one in the original contribution that was accepted at ACM MM.

---

# Packaging Guidelines

These packaging guidelines are meant to cover general cases. Please keep in mind that every individual case is slightly different.

## Environment

The authors should explicitly specify the operating system and tools that should be installed as the environment. Such specifications should include dependencies with specific hardware features (e.g., 25 GB of RAM are needed) or dependencies within the environment (e.g., the compiler that should be used must be run with a specific version of the operating system).

Note: The submitted artifacts should not require specific hardware and software that are hard to get access to. The committee will try to do the best to find reviewers who can setup the required environments; however, if no reviewer can be found, the companion paper cannot be accepted.

## System
System setup is one of the most challenging aspects when replicating experiments. The system setup will be easier to conduct if it is automatic rather than manual. Authors should test that the system they distribute can actually be installed in a new environment. The documentation should detail every step in the system setup:

- How to obtain the system?
- How to configure the environment if need be (e.g., environment variables, paths)?
- How to compile the system? (existing compilation options should be mentioned)
- How to use the system? (What are the configuration options and parameters to the system?)
- How to make sure that the system is installed correctly?

The above tasks should be achieved by executing a set of scripts provided by the authors that will download needed components (systems, libraries), initialize the environment, check that software and hardware is compatible, and deploy the system.

## Tools

The committee suggests that authors use [ReproZip](https://www.reprozip.org/) in order to streamline this process. ReproZip can be used to capture the environment, the input files, the expected output files, and the required libraries. A detailed how-to guide (installing, packing experiments, unpacking experiments) can be found in there. ReproZip will help both the authors and the evaluators to seamlessly rerun experiments.

If using ReproZip to capture the experiments proves to be difficult for a particular paper, the committee will work with the authors to find the proper solution based on the specifics of the paper and the environment needed.

---

# Badged Paper Examples

Here are a few links to papers with badges inside the ACM DL. They have been picked because they nicely illustrate what is described in this page, not due to their scientific value. They come from multiple domains, giving you a broad view of what colleagues have done. 

The first two papers from ACM MM 2019 comply with the guidelines specified on this page. The latter two do not necessarily comply with our rules. For example, sometimes there is no companion paper, sometimes the companion paper is not always a 2-4 page paper, ACM style. Furthermore, papers may target other types of badges that are about the availability of the artifacts, not solely about replicability or reproducibility of results.

* **Reproducibility Companion Paper: Knowledge Enhanced Neural Fashion Trend Forecasting.** Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng Chua, Jinyoung Moon, Hong-Han Shuai. __ACM ICMR 2021__. [Paper's DOI](https://dl.acm.org/doi/10.1145/3460426.3463598). The artifacts are on [Zenodo](https://zenodo.org/record/4774766#.YKdfH6LnhhE).
* **Reproducibility Companion Paper: Human Object Interaction Detection via Multi-level Conditioned Network**, Yunqing He, Xu Sun, Hui Jiang, Tongwei Ren, Gangshan Wu, Maria Sinziana Astefanoaei, Andreas Leibetseder. __ACM ICMR 2022__. [Paper's DOI](https://dl.acm.org/doi/10.1145/3512527.3531438). The artifacts are on [GitHub](https://github.com/ZhengyuZhao/Adi-Red-Scene).
* **Reproducible Experiments on Adaptive Discriminative Region Discovery for Scene Recognition**, Z. Zhao, Z. Liu, M. Larson, A. Iscen, N. Nitta. _ACM MM 2019_. ![](https://project.inria.fr/acmmmreproducibility/files/2018/09/results_replicated-150x150.jpg){:height="30px"}. [Paper's DOI](https://dl.acm.org/citation.cfm?id=3351169). The artifacts are on [GitHub](https://github.com/ZhengyuZhao/Adi-Red-Scene).
* **Generating Preview Tables for Entity Graphs**, N. Yan, S. Hasani, A. Asudeh and C. Li. _Winner of the 2017 most reproducible papers from SIGMOD 2016_. ![](https://project.inria.fr/acmmmreproducibility/files/2018/09/artifacts_available-150x150.jpg){:height="30px"}![](https://project.inria.fr/acmmmreproducibility/files/2018/09/results_replicated-150x150.jpg){:height="30px"}. [Paper&#8217;s DOI](https://doi.org/10.1145/2882903.2915221).

---

# References

Researchers in the area of databases started explicitly emphasizing reproducibility early on. 
Much of what we propose to implement for ACM Multimedia is inspired from their _DOs_ and _DON'Ts_. 

A good source of information can be found in the [ICDE 2008 tutorial by Ioana Manolescu and Stefan Manegold](http://pages.saclay.inria.fr/ioana.manolescu/SLIDES/ManolescuManegoldICDE2008.pdf). The tutorial includes a road map of tips and tricks on how to organize and present code that performs experiments so that an outsider can repeat them. A discussion about reproducibility in research including guidelines and a review of existing tools can be found in the [SIGMOD 2012 tutorial by Juliana Freire, Philippe Bonnet, and Dennis Shasha](http://dl.acm.org/citation.cfm?id=2213908). 

Closer to multimedia, ACM MMSys and ACM MM have led the multimedia research community in explicitly emphasizing reproducibility and Gwendal Simon chaired the reproducibility track at MMSys 2019. Emailing Gwendal or reading what he wrote about reproducibility [in his blog](http://peerdal.blogspot.com/2017/05/reproducibility-in-acm-mmsys-conference.html) can be helpful.

